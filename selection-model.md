## 选择模型(Selection Model)

具体到时空截面中的SPATEM生物智能是目前一种有效的与环境共存的智能体，这些智能体普遍具在环境中的任务执行能力。  
一般情况下智能体在一定时间范围内用于学习的资源是有限的，那么我们定义在有限资源上进行智能计算的智能体为限定智能体，同时对于非上帝智能体对于环境的观测及建立的参考固定化系统是有范围的。对于智能体范围限定的描述为限定窗口。对于用于只能计算的限定资源会约束智能体的能力范围，即限定离可以处理的OPPS任务集合大小。智能体在限定资源上执行的神经任务为智能行为，运行的神经任务信息变换与处理的系统为神经计算系统。智能体本体的生存时间为生命周期。  

限定SPATEM智能体的时空参考模型，受限于智能体的观测能力以及生命周期。智能体随着时间空间，环境的迁移，信息输入的概念环境随着时间空间的演变而演变。由于不同限定的智能体在时空上的基本上不存在位置和时间的同时重叠，因此智能体个体间存在输入信息差异。智能体客观存在环境在智能体的E3概念过程为直接环境映射。OPPS是针对一个任务的上的连续的OPPA集合，随着动作产生的客观环境的变化为演进环境。智能体主动或被动发现或者环境具备一个OPPS的执行条件为OPPS起点环境，对应的时间记为S(0)，智能体主动或被动的结束任务的为OPPS终点环境记为S(T)。  
SPATEM智能体对于一组OPPS信息集合足够敏感，并且能够稳定通过信号或者手段识别为概念形成(Confirm)。指出一组具有特征的信息集合观测为概念指出(Figure Out)。通过Opps将信息表达并可用于传输为概念表达（Represent）。  
OPPS的起点与终点环境是受智能体判断的，假定环境具备条件了但智能体不采取行动，那么这个环境尽管具备任务执行条件，但这个环境不是一个演进环境。因此起点与终点的选择由智能体的价值选择产生。智能体在判断一个环境的任务执行条件评估为OPPS起点价值判定过程。在执行过程中动作采取条件的评估为OPPS价值判定过程。  
在执行一个OPPS任务时可能面临多种结果，一个任务是否被成功执行不只是取决于环境的变化，也取决于自身的状态是否具备能力完成任务，或者OPPS的依赖项是否完备。对于OPPS完成的必要条件组成OPPS依赖价值链。同样SPATEM智能体本体及使用工具可以执行的动作的也是有限的，包括产生动作的部件或工具，以及每一个动作的时空影响。SPATEM所能产生的动作为OPPS动作集合。  
OPPS概念过程及概念模型需要的计算资源为计算代价，E3及概念过程中依赖的信息为OPPS信息集合。智能体在SPATEM从S(0)到S(T)的时间参考固定化为时间序列建模，在采样截面上的最小观测尺度上的连续建模为连续时序模型。由S(0)到S(T)的时间序列模型为OPPS的迹。价值判定过程的时间参考系上的计时为反应时间。智能体本体在当前观测时间邻域为t(0)。  
SPATEM智能体在随环境演化过程中，OPPS集合会随着环境的情况以及智能体自身的价值选择发生变化，某一些OPPS能力轨迹被价值遗忘采样，某一些探索或学习的能力轨迹得到强化。OPPS集合的遗弃与强化过程的参考模型为选择模型。  
SPATEM智能体发生的全量OPPS的环境为历史环境。无论环境是否有OPPS迹的存在，环境都是一种客观存在，含了不受OPPS影响的环境。在OPPS发生时概念过程E3表现为智能体对于历史环境的记忆。如某些智能体对附近时间历史的记忆更加精确，我们可以通过让智能体进行如概念指出的方式测试其对于历史的记忆精确程度称为指出测试，指出测试有效通常要求SPATEM智能体接受到指出测试时能够发生记忆追溯行为。在时序上的记忆为时序记忆模型，靠近发生时刻的历史环境为邻域环境，领域环境的记忆是一种局部记忆。对于某些智能体对于部分有价值的OPPS及历史环境有更精确的记忆，这通常由于是智能体价值的一种选择为价值记忆选择。智能体对历史环境的记忆也由于神经计算资源限制等原因而存在限制称为局限记忆。      
即便对于发生OPPS历史邻域环境的所有可以捕获的OPPS信息智能体也能存在非精确记忆，这些环境信息没有被选择记忆或者没有被选择观测，这称为价值记忆及价值观测。对于一些智能体而言通常在邻域附近的记忆信息量与精度随着时间靠近t0而增加，对于信息量与记忆精度的衡量为有效记忆密度。智能体之间的发生的关于历史OPPS概念传递为记忆传递。智能体记忆随时序模型参考系下的密度曲线为时间记忆曲线。    
具备默认内生故事线的SPATEM智能体为内驱智能体，内驱智能体通过内建的价值模型驱动其在演进环境中执行OPPS任务。内驱智能体主动执行的OPPS任务的过程为主动价值驱动，被动反应执行的OPPS任务的过程为被动价值驱动。由价值驱动的智力行为是价值驱动模型。智能体将历史记忆对应的概念集合形成符号记录，其载体为外部记忆载体。    
智能体在SPATEM的空间存在的参考固定化为智能本体的空间模型，本体所在空间当前位置的邻域为o(0)，智能体OPPA时刻在SPATEM的相对位置为x(0)，x(0)为智能体OPPS领域中心。在t(0)智能体的实体存在一个聚集位置邻域o(0)，在t(0)观测邻域与智能系统邻域动作影响邻域在o(0)邻域高度重合，那么智能体是一种OPPS同体具身智能体。在SPATEM参考模型中，所有智能体的OPPA模型需要满足截面逻辑模型，即OPPA的动作产生必然发生在t(0)邻域。    
SPATEM在时间模型的不同时间点，在空间的一个位置的领域的环境的变换情况为环境变化率，稳定截面的环境变化率较低，重复截面在时间上的环境变化及环境变化率呈现周期为环境周期。如果稳定截面的部分截面属性的相对变化接近于不变，那么这个为稳定截面属性为不变环境属性。智能体对于不变环境属性的观测的多次采样中，如果存在重复的属性信息为重叠采样。  
限制SPATEM智能本体OPPA模型对演进环境的采样也存在局限。OPPS迹对应的截面属性采样为OPPS观测迹，智能体在OPPS区域采样时，观测迹上不同时刻存在重复采样的截面信息为重叠信息，重叠信息的占比为不同时刻采样重叠率,智能体在故事线E3的环境采样简化了采样信息的概念重整过程为E3概念简化，此时故事线对应的概念环境为E3简化表示。智能体通过智能任务实现简化E3为神经简化重整。通过概念简化的记忆为简化记忆。通过多次采样对OPPS的伴随环境E3的概念模型为智能体对环境的认知，对于环境认知的完整程度为E3完整性，多次观测对于环境一致程度为E3一致性。  
智能群体的本体生命周期平均为平均生命周期。智能体群体在远超平均生命周期的尺度上，与族群相伴的OPPS演进环境也在发生变化，族群的OPPS集合也在不断的遗忘及更新。内驱智能体的价值模型与OPPS及演进环境邻域匹配的价值模型为t(0)价值模型。t(0)价值模型在t(0)邻域的动作符合故事线的价值目标对齐。  
存在故事线耦合的同类的行为的OPPS为耦合OPPS，如果耦合OPPS对应的多个个体完成正向目标时，群体的参与个体的价值对齐也为正，那么为协作OPPS。协作OPPS亦可以称作协作任务。群体需要通过概念共享完成OPPS协作过程中的信息传递。并且通过信息交流多个个体执行不同OPPS子任务达到目标，将多个个体的OPPS耦合完成的一个全局的OPPS为一种OPPS程序，OPPS的子任务的执行为一种程序化执行。 协同OPPS当前智能体外的其他的OPPS的任务执行本体的时空位置与x(0)不重叠，尽管其他智能体的OPPS可能会收该智能体影响，但其他的OPPS执行要素{O,P,P,S}不受x(0)本体控制。智能体依据x(0)执行{O,P,P,S}，对于环境观测预测及任务及动作规划的变现为智能体的行动意志。   
智能体对环境的预测规划的时间长度为规划时长，真对于事件的规划为事件规划，真对于行动的规划为行动规划。如果群体OPPS的全局规划为具体的一个体为群OPPS规划主导。其他的个体执行配合主导规划下的OPPS，子OPPS的规划为局部规划时完成主导规划。主导规划的目标为群体的行动意志。  
设定获取群体目标的外部环境发生变化，群体主导OPPS无法预测环境的变化如不可预测的随机性事件等，对应的演进环境为不可预测环境，如果OPPS执行的时的演进环境可以预测，那么为可预测环境，对于预测环境的的事件长度或者逻辑长度为可预测长度。如果一个OPPS从开始到结束的环境可预测，则为可预测OPPS，如果OPPS的环境和故事线均可预测且可规划，演进环境与OPPS故事线构成确定性局面。如果一个OPPS及演进环境，执行相同的OPPS时得到一致结果，则为可重入环境。由于智能体属于环境的一部分，智能体对于环境采取动作能导致环境改变，确定性局面的故事线随着预期动作的发生环境的变换发生预期的变化，如果这个动作的产生可以是具备产生该动作能力的不同的智能体，那么这种情况下该局面为客观确定性局面。假定客观确定性局面的每一个动作和故事线的匹配都是按照提前制作的动作序列构成，这些动作序列为脚本，这个局面为客观程序化局面。  
如智能体第一次完成确定性局面OPPS时，尽管确定环境故事线客观可预测，但对于智能体第一次接触的局面存在不能预测的可能。在多次观测环境及执行OPPS任务过程中通过记忆模型，在后续执行改确定局面OPPS时，可以完成对局面的预测为基于记忆的预测。智能体对客观可预测的演进环境及OPPS为智能体可预测。完全基于观测进行预测规划为基于观测的OPPS，智能体基于E3的概念模型和观测进行的OPPS为基于E3的OPPS，为了完成任务对于环境的必要观测和预测为必要观测，需要必要观测时长为观测时长，必要的预测时长为预测时长。如果随着智能体多次执行同类OPPS对于伴随环境的E3的完备性逐步提高，为E3成熟过程，对应表征E3的概念模型的信息一致完备性和一致性曲线为E3成熟曲线，E3成熟对应的记忆过程为E3记忆模型。智能体基于无E3参与或者不同成熟度的E3概念模型实现的OPPS具有不同的执行及成功效率，如果基于E3的OPPS加速了执行成功率以及提升了执行效率为基于E3快速反应通路。  
群智能体在执行OPPS群体行动意志时，外部环境的确定性，可能导致在不同的协同智能体有不同的反应。假定群体行动OPPS的协作目标是协作OPPS的目标是相同，目标是通过某个单一智能体获得结果，一旦某智能体完成目标则群体目标达成，目标对于不同的智能体呈现随机出现，这个OPPS环境是不可确定预测的环境，假设智能体通过概率等基于模型的预测，由于随机性存在，那么存在预测失败的可能。群体在OPPS的主导可能依据预测的目标执行进行资源配置，但为了确保目标完成设置了备选智能体执行协作OPPS。依据模型的预测的OPPS执行为基于模型执行，在执行过程中由于环境的变化以及模型预测的不精确性做出的调整为OPPS的规划调整。当群体在执行任务预测规划为A智能体最有可能完成任务进行配置，B智能体做备选协作执行；但实际情况时B发生依据备选情况完成任务；那么B经过B执行的任务不需要重新配置资源，不需要重新规划（不因为重新规划失去完成OPPS的时空机会）反应，那么B智能体对应的OPPS可以不受A智能体OPPS关联，可以称为是程序性快速反应通路。  
有生命的本体不一定具有智能，但通常的碳基智能体具有生命。没有智能的生命体在适合演进环境中依然可以进行生命的故事线活动，无智能体的内生故事线OPPS为基于脚本的无主观程序OPPS，OPPS的目标为无主观目标。碳基本体存在多种不同的形态来完成无主观目标，不同的形态对应不同的无主观脚本，尽管脚本存在差异，但脚本的目标可以相同。通过脚本多样性实现无主观目标为脚本内置程序实现无主观目标。碳基智能体则可以使用智能系统实现无主观目标。对于本体实现无主观目标的能力不同，在环境中的影响力和本体视角的对环境的影响能力也不同，由于影响力和能力的区别也间接导致无主观的具体目标不同为目标差异性。本体的统目标可以共同描述的无主观目标为共同本能目标。本体在不同的能力环境需要实现的具体的差异目标为差异本能目标。智能本体拥有智能系统实现的共同本能目标是本体脚本差异多样化的一个本能目标之一，本体智能系统增加的影响力以及环境适应能力为目标差异下的特性增强。本体拥有了智能系统提高的共同本能目标的完成效率与成功率为通过外部系统实现无主观目标能力增强。为无主观目标服务的智能系统活动为智能系统的本能OPPS故事线。
